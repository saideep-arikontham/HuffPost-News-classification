{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a705fdd0-dc66-4388-996a-7d551eb62350",
   "metadata": {},
   "source": [
    "# Generating Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c661f2f-1e51-4741-8abf-2dfd359f0e41",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f15b9c-52e4-4c5d-af54-25a7d2b911be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78126c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import fasttext\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3e6d6d7-f0ed-46f8-96ac-e6d39ec40933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saideepbunny/Projects/HuffPost-News-classification\n"
     ]
    }
   ],
   "source": [
    "path = Path(os.path.dirname(os.getcwd()))\n",
    "path = str(path)\n",
    "print(path)\n",
    "sys.path.insert(1, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8394b13-3eb7-471c-9e5a-87d4b86ce98a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saideepbunny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saideepbunny/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/saideepbunny/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils.utils import preprocess_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5b143d-0283-4746-b04a-83b159f801f8",
   "metadata": {},
   "source": [
    "## Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af3a9e4-1521-4756-a989-4f88bd26ad11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>headline</th>\n",
       "      <th>category</th>\n",
       "      <th>short_description</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>headline_length</th>\n",
       "      <th>short_description_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stephen Colbert Hits Trump With The Perfect 'S...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>\"Late Show\" host has a correction for the pres...</td>\n",
       "      <td>Ed Mazza</td>\n",
       "      <td>2018-01-12</td>\n",
       "      <td>63</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dear Colleagues: We SUCK!</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>I'm not saying we are wrong. In fact, we may a...</td>\n",
       "      <td>David Katz, M.D., ContributorFounder, True Hea...</td>\n",
       "      <td>2015-03-10</td>\n",
       "      <td>25</td>\n",
       "      <td>220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Emily Fletcher Shares Guided Meditation Techni...</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Meditation doesn't have to be complicated. In ...</td>\n",
       "      <td>None</td>\n",
       "      <td>2013-11-28</td>\n",
       "      <td>57</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HuffPost Rise: What You Need To Know On Februa...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Welcome to the HuffPost Rise Morning Newsbrief...</td>\n",
       "      <td>None</td>\n",
       "      <td>2016-02-15</td>\n",
       "      <td>51</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3 Ways To Fight Overwhelm And Add Joy To Your ...</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>Working moms are juggling more than ever befor...</td>\n",
       "      <td>Paula Jenkins, ContributorLife Coach and Host ...</td>\n",
       "      <td>2016-07-09</td>\n",
       "      <td>50</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61970</th>\n",
       "      <td>Owning An Assault Weapon Is No Longer A Fundam...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>An appeals court wiped out an earlier ruling t...</td>\n",
       "      <td>Cristian Farias</td>\n",
       "      <td>2016-03-06</td>\n",
       "      <td>68</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61971</th>\n",
       "      <td>Blackfish: Rooting for Killer Whales and Kille...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>A debate ensued that resonates for yet another...</td>\n",
       "      <td>Regina Weinreich, Contributor\\nAuthor, 'Keroua...</td>\n",
       "      <td>2013-06-22</td>\n",
       "      <td>69</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61972</th>\n",
       "      <td>Trevor Noah Mockingly Praises Trump's 'Right R...</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>\"Now I know your first instinct is to be disgu...</td>\n",
       "      <td>Lee Moran</td>\n",
       "      <td>2017-11-28</td>\n",
       "      <td>73</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61973</th>\n",
       "      <td>Elite 'Bundlers' Raise More Than $113 Million ...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>Big backers include Ben Affleck, George Lucas,...</td>\n",
       "      <td>Michael Beckel, Center for Public Integrity</td>\n",
       "      <td>2016-09-23</td>\n",
       "      <td>65</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61974</th>\n",
       "      <td>Alex Jones Is Finally Getting The Defamation L...</td>\n",
       "      <td>POLITICS</td>\n",
       "      <td>The Infowars host spun a conspiracy around a w...</td>\n",
       "      <td>Andy Campbell</td>\n",
       "      <td>2018-03-13</td>\n",
       "      <td>64</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61975 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                headline       category  \\\n",
       "0      Stephen Colbert Hits Trump With The Perfect 'S...  ENTERTAINMENT   \n",
       "1                              Dear Colleagues: We SUCK!       WELLNESS   \n",
       "2      Emily Fletcher Shares Guided Meditation Techni...       WELLNESS   \n",
       "3      HuffPost Rise: What You Need To Know On Februa...       POLITICS   \n",
       "4      3 Ways To Fight Overwhelm And Add Joy To Your ...       WELLNESS   \n",
       "...                                                  ...            ...   \n",
       "61970  Owning An Assault Weapon Is No Longer A Fundam...       POLITICS   \n",
       "61971  Blackfish: Rooting for Killer Whales and Kille...  ENTERTAINMENT   \n",
       "61972  Trevor Noah Mockingly Praises Trump's 'Right R...  ENTERTAINMENT   \n",
       "61973  Elite 'Bundlers' Raise More Than $113 Million ...       POLITICS   \n",
       "61974  Alex Jones Is Finally Getting The Defamation L...       POLITICS   \n",
       "\n",
       "                                       short_description  \\\n",
       "0      \"Late Show\" host has a correction for the pres...   \n",
       "1      I'm not saying we are wrong. In fact, we may a...   \n",
       "2      Meditation doesn't have to be complicated. In ...   \n",
       "3      Welcome to the HuffPost Rise Morning Newsbrief...   \n",
       "4      Working moms are juggling more than ever befor...   \n",
       "...                                                  ...   \n",
       "61970  An appeals court wiped out an earlier ruling t...   \n",
       "61971  A debate ensued that resonates for yet another...   \n",
       "61972  \"Now I know your first instinct is to be disgu...   \n",
       "61973  Big backers include Ben Affleck, George Lucas,...   \n",
       "61974  The Infowars host spun a conspiracy around a w...   \n",
       "\n",
       "                                                 authors       date  \\\n",
       "0                                               Ed Mazza 2018-01-12   \n",
       "1      David Katz, M.D., ContributorFounder, True Hea... 2015-03-10   \n",
       "2                                                   None 2013-11-28   \n",
       "3                                                   None 2016-02-15   \n",
       "4      Paula Jenkins, ContributorLife Coach and Host ... 2016-07-09   \n",
       "...                                                  ...        ...   \n",
       "61970                                    Cristian Farias 2016-03-06   \n",
       "61971  Regina Weinreich, Contributor\\nAuthor, 'Keroua... 2013-06-22   \n",
       "61972                                          Lee Moran 2017-11-28   \n",
       "61973        Michael Beckel, Center for Public Integrity 2016-09-23   \n",
       "61974                                      Andy Campbell 2018-03-13   \n",
       "\n",
       "       headline_length  short_description_length  \n",
       "0                   63                        52  \n",
       "1                   25                       220  \n",
       "2                   57                       120  \n",
       "3                   51                       103  \n",
       "4                   50                       123  \n",
       "...                ...                       ...  \n",
       "61970               68                        94  \n",
       "61971               69                       121  \n",
       "61972               73                       125  \n",
       "61973               65                        81  \n",
       "61974               64                       116  \n",
       "\n",
       "[61975 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(f'{path}/data/train_data.json', orient='records')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a127503-8d14-4233-879a-54990eef31eb",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc57b087-086a-4045-b2d5-abebd0b7d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['headline'] + df['short_description']\n",
    "df['content_preprocessed'] = df['content'].apply(lambda x: preprocess_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0e93f3-ea2e-4db5-a71a-0fca31442422",
   "metadata": {},
   "source": [
    "## Creating Embedding models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3190c0f-1b92-4664-9b7e-3d7661ab490f",
   "metadata": {},
   "source": [
    "### CBOW models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15030a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cbow(embedding_dim, words, window, epochs, sg):\n",
    "    '''\n",
    "    creating word2vec or skipgram.\n",
    "    '''\n",
    "    #Creating Word2Vec\n",
    "    cbow_model = gensim.models.Word2Vec(words , vector_size = embedding_dim , window = window , min_count = 1, epochs = epochs, sg = sg)\n",
    "    created_model = \"WORD2VEC\" if sg==0 else \"SKIPGRAM\"\n",
    "    \n",
    "    print(f'{created_model} {embedding_dim} VECTOR EMBEDDING DIMENSIONS:')\n",
    "    print(f'=========================================')\n",
    "    print(f'- Vocabulary count: {len(cbow_model.wv)}')\n",
    "    print(f'''- Similar words for word \"great:\\n\"{cbow_model.wv.most_similar('great')}''')\n",
    "    print('\\n')\n",
    "    \n",
    "    cbow_model.save(f\"{path}/embeddings/{created_model.lower()}_model_{embedding_dim}.model\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b0ab16-c5d0-4edb-a10d-9bc82b9c94d3",
   "metadata": {},
   "source": [
    "### Fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4650b5e-6b2d-4eb2-8493-eecc5352f998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#writing the text column to build embeddings\n",
    "train_content_path = f'{path}/data/text_label.txt'\n",
    "df.to_csv(train_content_path, columns = ['content_preprocessed'], header = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "462ae918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fasttext(train_content_path, embedding_dim):\n",
    "    \n",
    "    model = fasttext.train_unsupervised(train_content_path, dim = embedding_dim)\n",
    "    \n",
    "    print(f'FASTTEXT {model.dim} VECTOR EMBEDDING DIMENSIONS:')\n",
    "    print(f'=========================================')\n",
    "    \n",
    "    print('- Fasttext embeddings Created')\n",
    "    print(f'- Vocabulary count: {len(model.words)}')\n",
    "    print(f'''- Similar words for word \"great:\\n\"{model.get_nearest_neighbors('great', k=10)}''')\n",
    "    \n",
    "    model.save_model(f'{path}/embeddings/fasttext_model_{embedding_dim}.bin')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4e1e6a-c67b-4a0e-a3bf-34d14a350d47",
   "metadata": {},
   "source": [
    "## Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf274819-8980-40da-994d-6787c5e22701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD2VEC 100 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Vocabulary count: 80421\n",
      "- Similar words for word \"great:\n",
      "\"[('good', 0.4518512785434723), ('wonderful', 0.4399999976158142), ('certainly', 0.4133574366569519), ('medicate', 0.3924970328807831), ('againonly', 0.3861885368824005), ('art', 0.3858899772167206), ('sure', 0.38400423526763916), ('sellout', 0.38033515214920044), ('hurtall', 0.3745027482509613), ('channel', 0.37382563948631287)]\n",
      "\n",
      "\n",
      "SKIPGRAM 100 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Vocabulary count: 80421\n",
      "- Similar words for word \"great:\n",
      "\"[('goalsponsors', 0.6539395451545715), ('evernote', 0.6371009945869446), ('challengesnow', 0.6287484765052795), ('finishers', 0.6078079342842102), ('narcissismthe', 0.6032633781433105), ('nationalistmake', 0.5986570715904236), ('againpeople', 0.59730064868927), ('obamathere', 0.5955760478973389), ('establishes', 0.5935031175613403), ('dreamsbeing', 0.5898616909980774)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  17267\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   85796 lr:  0.000000 avg.loss:  2.305209 ETA:   0h 0m 0s 98.3% words/sec/thread:   86148 lr:  0.000867 avg.loss:  2.307111 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTTEXT 100 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Fasttext embeddings Created\n",
      "- Vocabulary count: 17267\n",
      "- Similar words for word \"great:\n",
      "\"[(0.8244670033454895, 'greats'), (0.7737111449241638, 'greasy'), (0.720535933971405, 'greatly'), (0.7141452431678772, 'greatest'), (0.7092286348342896, 'greatist'), (0.7041301131248474, 'greatness'), (0.6738501787185669, 'create'), (0.6681212782859802, 'greater'), (0.6251865029335022, 'creates'), (0.604482889175415, 'goodwill')]\n",
      "\n",
      "\n",
      "**********************************************************************************************************************\n",
      "\n",
      "\n",
      "WORD2VEC 200 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Vocabulary count: 80421\n",
      "- Similar words for word \"great:\n",
      "\"[('exceptional', 0.34178149700164795), ('good', 0.33802881836891174), ('attractors', 0.318238765001297), ('magical', 0.31763002276420593), ('sacred', 0.2883853018283844), ('channel', 0.2840352952480316), ('learning', 0.2821224331855774), ('surprising', 0.2805941700935364), ('difficult', 0.27612173557281494), ('lots', 0.2755577564239502)]\n",
      "\n",
      "\n",
      "SKIPGRAM 200 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Vocabulary count: 80421\n",
      "- Similar words for word \"great:\n",
      "\"[('evernote', 0.535286545753479), ('goalsponsors', 0.5262601971626282), ('hugwith', 0.48340943455696106), ('iqtell', 0.4813786745071411), ('superhumanly', 0.47991684079170227), ('coupon', 0.47854575514793396), ('againpeople', 0.4784628450870514), ('entitles', 0.4767494797706604), ('challengesnow', 0.4750047028064728), ('finishers', 0.47411656379699707)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  17267\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   48627 lr:  0.000000 avg.loss:  2.309833 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTTEXT 200 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Fasttext embeddings Created\n",
      "- Vocabulary count: 17267\n",
      "- Similar words for word \"great:\n",
      "\"[(0.8348000049591064, 'greats'), (0.7866116166114807, 'greasy'), (0.7409617304801941, 'greatly'), (0.7353630661964417, 'greatest'), (0.7262373566627502, 'greatist'), (0.700736939907074, 'greatness'), (0.6940997838973999, 'greater'), (0.6416086554527283, 'grease'), (0.6340741515159607, 'create'), (0.6006224751472473, 'creates')]\n",
      "\n",
      "\n",
      "**********************************************************************************************************************\n",
      "\n",
      "\n",
      "WORD2VEC 300 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Vocabulary count: 80421\n",
      "- Similar words for word \"great:\n",
      "\"[('exceptional', 0.300202339887619), ('housei', 0.2871813476085663), ('wonderful', 0.27409595251083374), ('attractors', 0.27407634258270264), ('magical', 0.26589444279670715), ('sacred', 0.2656116187572479), ('industrya', 0.2568259537220001), ('honoring', 0.25417360663414), ('stressful', 0.24914678931236267), ('buzz', 0.24297405779361725)]\n",
      "\n",
      "\n",
      "SKIPGRAM 300 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Vocabulary count: 80421\n",
      "- Similar words for word \"great:\n",
      "\"[('evernote', 0.4372881054878235), ('goalsponsors', 0.4236864447593689), ('coupon', 0.4190019965171814), ('iqtell', 0.4138096570968628), ('entitles', 0.4118344783782959), ('hugwith', 0.4066886603832245), ('againtiana', 0.4019538462162018), ('challengesnow', 0.39732879400253296), ('finishers', 0.39286044239997864), ('immigrantslahren', 0.39221280813217163)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  17267\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:   29220 lr:  0.000000 avg.loss:  2.307586 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FASTTEXT 300 VECTOR EMBEDDING DIMENSIONS:\n",
      "=========================================\n",
      "- Fasttext embeddings Created\n",
      "- Vocabulary count: 17267\n",
      "- Similar words for word \"great:\n",
      "\"[(0.8409676551818848, 'greats'), (0.7794288396835327, 'greasy'), (0.7383869886398315, 'greatly'), (0.7345160841941833, 'greatest'), (0.7273334860801697, 'greatist'), (0.7152729630470276, 'greatness'), (0.6994662284851074, 'greater'), (0.6535100936889648, 'grease'), (0.6266390681266785, 'create'), (0.6094589233398438, 'retreat')]\n",
      "\n",
      "\n",
      "**********************************************************************************************************************\n",
      "\n",
      "\n",
      "CPU times: user 33min 23s, sys: 19.9 s, total: 33min 43s\n",
      "Wall time: 10min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "embedding_dims = [100, 200, 300]\n",
    "corpus = [word_tokenize(sentence) for sentence in df['content_preprocessed']]\n",
    "\n",
    "for dim in embedding_dims:\n",
    "\n",
    "    #Word2Vec\n",
    "    create_cbow(dim, corpus, 5, 50, 0)\n",
    "\n",
    "    #Skipgram\n",
    "    create_cbow(dim, corpus, 5, 50, 1)\n",
    "\n",
    "    #Fasttext\n",
    "    create_fasttext(train_content_path, dim)\n",
    "\n",
    "    print('\\n\\n**********************************************************************************************************************\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (huffpost_env)",
   "language": "python",
   "name": "huffpost_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
